from collections import Counterimport scipy.stats as statsimport networkx as nximport matplotlib.pyplot as pltfrom scipy.stats import stats, spearmanrfrom Processing import *class Visualiser:    def __init__(self, hypergraph):        self.hypergraph = hypergraph        self.last_sale_prices = get_last_sale_price_dict(self.hypergraph.collection)        self.average_prices = get_average_price_dict(self.hypergraph.collection)        # Nodes can be coloured by nr of trades, last_sale and part ranges (only for by_price)    def draw_graph(self, node_color_type, edges_visible=False):        if not 2 <= self.hypergraph.nr_parts <= 4:            raise Exception("Number of parts considered should be between 2 and 4 for efficiency reasons.")        if node_color_type == COLOR_NODES_PART_PRICE and not self.hypergraph.by_price:            raise Exception("Coloring by the part number only for partitioning by price.")        G = nx.Graph()        cnt_edges = 0        positions = dict()  # positions of the nodes (for graph drawing)        part_coord = dict()  # coordinates of the k_part parts        part_net = dict()  # dict of (part, nets)        # Get the centers of the parts        dx, dy = helper_get_part_center(self.hypergraph.nr_parts, self.hypergraph.by_price)        # Preserve ordering of vertices for their color        int_nets = self.hypergraph.int_nets.keys()        ext_nets = self.hypergraph.ext_nets.keys()        vertices = self.hypergraph.vs_subset        # Add nodes for the vertices set (NFTs) and internal and external nets (traits)        G.add_nodes_from(vertices)        G.add_nodes_from(int_nets)        # External nets visible only for the partitioning produced by PaToH        if not self.hypergraph.by_price:            G.add_nodes_from(ext_nets)        # Get the nodes colours and the legends for visualisation        if node_color_type == COLOR_NODES_NR_TRADES:            nodes_color, legend = get_nodes_color_by_nr_trades_dict(self.hypergraph.collection)        elif node_color_type == COLOR_NODES_LAST_SALE:            nodes_color, legend = get_nodes_color_by_last_sale_price_dict(self.hypergraph.collection)        else:  # Color by part ranges (only for by_price)            nodes_color = dict()            legend = dict()            for p in range(self.hypergraph.nr_parts):                part_prices = [self.last_sale_prices[v] for v in self.hypergraph.part_vert_filtered[p]]                min_price = float("{:.2f}".format(min(part_prices)))                max_price = float("{:.2f}".format(max(part_prices)))                for v in self.hypergraph.part_vert_filtered[p]:                    nodes_color[v] = COLOR_DICT[p]                legend[p] = (min_price, max_price)        nodes_color_map = [nodes_color[v] for v in vertices]        # Add internal nets links/edges to parts        for n in int_nets:            vs = self.hypergraph.int_nets[n]            for v in vs:                G.add_edge(n, v)                cnt_edges += 1        # Add external nets links/edges to parts        if not self.hypergraph.by_price:            for n in ext_nets:                vs = self.hypergraph.ext_nets[n]                for v in vs:                    cnt_edges += 1                    G.add_edge(n, v)        print("\n")        # Draw a subgraph for the NFTs of each part        for p in range(self.hypergraph.nr_parts):            # Print details of the partitioning            # print("Part " + str(p + 1) + " has center: (" + str(dx[p]) + ", " + str(dy[p]) + "). Total nr of elements "            # + str(len(self.hypergraph.part_vert_filtered[p])) + ": " + str(self.hypergraph.part_vert_filtered[p]))            print("Part " + str(p + 1) + " has " + str(len(self.hypergraph.part_vert_filtered[p]))                  + " elements: " + str(self.hypergraph.part_vert_filtered[p]))            if self.hypergraph.by_price:                if self.hypergraph.price_type == NFT_PRICE_AVG_SALES:                    part_prices = [self.average_prices[v] for v in self.hypergraph.part_vert_filtered[p]]                else:                    part_prices = [self.last_sale_prices[v] for v in self.hypergraph.part_vert_filtered[p]]                print("----> Min and max prices in the part: " + str(min(part_prices)) + ", " + str(                    max(part_prices)) + "\n")            # Draw the subgraphs for the part            pos = helper_draw_subgraph_around((dx[p], dy[p]), PART_VERTICES, self.hypergraph.part_vert_filtered[p], G)            positions.update(pos)            part_coord[p] = p        print("\n")        # Compute the internal nets for each part        for n in int_nets:            v = self.hypergraph.int_nets[n][0]  # Internal vertices of the net will all have the same part            curr_part = self.hypergraph.vert_part[v]            if curr_part not in part_net.keys():                part_net[curr_part] = []            part_net[curr_part].append(n)        # For each part set the internal nets on a circle        for p in part_net.keys():            nets = part_net[p]            pos = helper_draw_subgraph_around((dx[part_coord[p]], dy[part_coord[p]]), PART_RADIUS_INT, nets, G)            nodes_color_map += [COLOR_INT_NET] * len(nets)  # Color the internal nets            positions.update(pos)        # Add a subgraph circle for the external nets, in the center        pos = helper_draw_subgraph_around(CANVAS_CENTER, PART_RADIUS_EXT, ext_nets, G)        positions.update(pos)        # Add the color of external nets        if not self.hypergraph.by_price:            nodes_color_map += [COLOR_EXT_NET] * len(ext_nets)        # Set the edge colours (invisible or gray)        edge_color_map = [WHITE_COLOR] * cnt_edges if not edges_visible else [GRAY_COLOR_SOFT] * cnt_edges        # Plot the figure        ax = plt.gca()        # f = plt.figure(1)        # ax = f.add_subplot(1, 1, 1)        nx.draw(G, pos=positions, edge_color=edge_color_map, node_color=nodes_color_map, with_labels=True, ax=ax)        # Add patches around the parts and their part numbers in the center        for i in range(self.hypergraph.nr_parts):            circle = plt.Circle(xy=(dx[i], dy[i]), radius=PART_RADIUS_INT, color=COLOR_PATCH)            ax.add_patch(circle)            plt.Axes.text(self=ax, x=dx[i] - 0.05, y=dy[i] - 0.05, s="Part " + str(i + 1))        # Construct the legent and make it visible        helper_construct_legend(node_color_type, self.hypergraph.is_price_filtered, self.hypergraph.by_price, legend,                                ax)        plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)        plt.show()    # Plots the most frequent traits for the NFTs within the price range considered    def draw_most_frequent_traits_price_partitioning(self, nr):        if not self.hypergraph.by_price:            raise Exception("Frequency of traits is computed only for price partitioning.")        traits_probs = get_traits_probability(self.hypergraph.collection)        traits_presence = get_trait_presence_in_price_analysis_dict(self.hypergraph.collection)        traits_count = get_traits_count(self.hypergraph.collection)        traits_sorted_by_prob = sorted(traits_probs.keys(), key=lambda n: traits_probs[n])        # Assign ranks to traits, where rank 1 = rarest wrt trait probability        ranks = dict()        rank = 1        for t in traits_sorted_by_prob:            ranks[t] = rank            rank += 1        # Compute the frequency of traits (nr of NFTs with the trait/nr of NFTs with the trait analysed)        percentages = dict()        for n in self.hypergraph.col_net_rep_filtered.keys():            perc = len(self.hypergraph.col_net_rep_filtered[n]) / traits_presence[n]            percentages[n] = perc        # Sort the traits by their presence percentage        sorted_by_perc = sorted(percentages.keys(), key=lambda n: percentages[n], reverse=True)        if not len(sorted_by_perc) >= nr:            raise Exception("The number provided " + str(nr) + " exceeds total number of traits.")        # Keep only the given number of traits        sorted_by_perc = sorted_by_perc[:nr]        cnt = 0        for n in sorted_by_perc:            cnt += 1            t_type, t_value = get_trait_for_consec_idx(n, self.hypergraph.collection)            label_str = t_value + " (" + t_type + ") - count " + str(traits_presence[n]) + ((" (" + str(                traits_count[n]) + ")") if self.hypergraph.collection == CRYPTOPUNKS else '') + ", rank " + str(                ranks[n])            plt.bar(cnt, percentages[n] * 100, label=label_str)        plt.xticks([])        plt.yticks(fontsize=15)        plt.ylabel("% of the number of NFTs with the trait", fontsize=15)        plt.title('Most frequent traits for the NFTs of the range considered', fontsize=15)        plt.legend(bbox_to_anchor=(1.05, 1.0), fontsize=15)        #plt.tight_layout()        plt.show()    # Plots rarity vs price of NFTs    # Rarity is generalised mean/p-norm and price last sale price or average sale price (base 10 logarithm)    # Kendall and Spearman coefficients are also computed    def draw_price_rarity_distribution_PaToH(self, special_subset, vs_subset, price_type, p_norm, apply_log,                                             divide_by_n,                                             infinity):        if not self.hypergraph.is_price_filtered:            raise Exception("In the rarity price distribution the NFTs without prices must be filtered out.")        if not all([v in self.last_sale_prices.keys() for v in vs_subset]):            raise Exception("The subset considered contains NFTs without price.")        if not all([v in vs_subset for v in special_subset]):            raise Exception("The specific subset given must be itself a subset of the vertices considered.")        if self.hypergraph.by_price:            raise Exception("Visualisation available only for the partitioning produced by PaToH.")        p_norms_all = get_p_norm_for_collection(self.hypergraph.collection, p=p_norm, apply_log=apply_log,                                                divide_by_n=divide_by_n, infinity=infinity)        parts = self.hypergraph.nr_parts        # If special subset not empty, then it will appear in the computation and in the legend        if len(special_subset) != 0:            parts += 1        # For each part plot the points        for i in range(parts):            if i == self.hypergraph.nr_parts:  # current subset is the special subset                curr_part_vs = special_subset            else:  # current subset is part of the parts resulted                curr_part_vs = [v for v in vs_subset if                                v in self.hypergraph.vs_subset and self.hypergraph.vert_part_filtered[v] == i]            # Get the p-norms with value in ascending order            sorted_vs_by_p_norms = sorted(curr_part_vs, key=lambda v: p_norms_all[v])            p_norms = [p_norms_all[v] for v in sorted_vs_by_p_norms]            # Get the prices with values in the order set by p-norms            vs_prices = []            for v in sorted_vs_by_p_norms:                if price_type == NFT_PRICE_LAST_SALE:                    vs_prices.append(log10(self.last_sale_prices[v]))                else:  # average of all sales                    vs_prices.append(log10(self.average_prices[v]))            # Draw the points with the labels            if i == self.hypergraph.nr_parts:                plt.scatter(p_norms, vs_prices, label="Specific subset")            else:                plt.scatter(p_norms, vs_prices, label="Part " + str(i + 1))            print("\nPart " + str(i + 1))            # Compute the correlation coefficients for the vertices of current part            calculate_correlation_Kendall(p_norms, vs_prices)            calculate_correlation_Spearman(p_norms, vs_prices)            # Draw line best estimating the points (rarity, price)            p = np.polyfit(p_norms, vs_prices, 1)            x_vals = np.linspace(min(p_norms), max(p_norms), 100)            y_vals = p[0] * x_vals + p[1]            plt.plot(x_vals, y_vals, label=f"y={p[0]:.2f}*x+{p[1]:.2f}")        # Compute the correlation coefficients for all vertices        all_vs = [v for v in vs_subset if v in self.hypergraph.vs_subset]        sorted_all_vs_by_p_norms = sorted(all_vs, key=lambda v: p_norms_all[v])        all_p_norms = [p_norms_all[v] for v in sorted_all_vs_by_p_norms]        if price_type == NFT_PRICE_LAST_SALE:            all_prices = [log10(self.last_sale_prices[v]) for v in sorted_all_vs_by_p_norms]        else:            all_prices = [log10(self.average_prices[v]) for v in sorted_all_vs_by_p_norms]        print("\nAll vertices:")        calculate_correlation_Kendall(all_p_norms, all_prices)        calculate_correlation_Spearman(all_p_norms, all_prices)        mean_type = helper_determine_mean_for_rarity_distributon(p_norm=p_norm, divide_by_n=divide_by_n, apply_log=apply_log, infinity=infinity)        if mean_type == GEOMETRIC_MEAN:            plt.xlabel("Negative logarithm of the geometric mean (p=0)", fontsize=15)        else:            plt.xlabel("Rarity", fontsize=15)        if price_type == NFT_PRICE_LAST_SALE:            plt.ylabel("Log base 10 of last sale price", fontsize=15)        else:            plt.ylabel("Log base 10 of average sale price", fontsize=15)        plt.title("Rarity and price of NFTs", fontsize=15)        plt.yticks(fontsize=15)        plt.xticks(fontsize=15)        plt.legend(loc=4, fontsize=15)        plt.show()    # Draw boxplots with trait probabilities for each connectivity    def draw_boxplots_conn_prob_PaToH(self):        if self.hypergraph.by_price:            raise Exception("Visualisation available only for the partitioning produced by PaToH.")        conns = self.hypergraph.net_conn        prob_traits_dict = get_traits_probability(self.hypergraph.collection)        traits_number = get_traits_number(self.hypergraph.collection)        x_values = [[] for _ in range(self.hypergraph.nr_parts)]        # Construct the list of probabilities for each net connectivity        for conn in range(self.hypergraph.nr_parts):  # part i has connectivity (i + 1)            for trait_idx in range(traits_number):                if conns[trait_idx] == conn + 1:                    prob = log10(prob_traits_dict[trait_idx])                    x_values[conn].append(prob)        fig, ax = plt.subplots(1)        bp = ax.boxplot(x_values, patch_artist=True)        # Set colour of the patches        for patch in bp['boxes']:            patch.set_facecolor(COLOR_PATCH_BOXPLOT)        # Set the colour of the median values        for median in bp['medians']:            median.set(color=COLOR_MEDIAN_BOXPLOT, linewidth=1)        ax.set_xticks([i + 1 for i in range(self.hypergraph.nr_parts)], fontsize=15)        plt.xticks(fontsize=13)        plt.yticks(fontsize=13)        ax.yaxis.grid(True)        ax.set_xlabel('Net connectivity', fontsize=15)        ax.set_ylabel('Log base 10 of trait probability', fontsize=15)        plt.title('Traits probabilities for each net connectivity for ' + str(self.hypergraph.get_name_str()),                  fontsize=15)        plt.show()    # Shows the net connectivities in each part    # if is_unique, net connectivity is counted only once in each part    def draw_histograms_of_connectivities_PaToH(self, is_unique):        if self.hypergraph.by_price:            raise Exception("Visualisation available only for the partitioning produced by PaToH.")        rep_dict = get_rep_dict_consec(self.hypergraph.collection)        Y_axis = [str(i + 1) for i in range(self.hypergraph.nr_parts)]        conns_counters = []        # Compute the connectivity distributions in each part        for i in range(self.hypergraph.nr_parts):            if not is_unique:  # counts all the net connectivities                conn_counter = Counter(                    [self.hypergraph.net_conn[t_idx] for v in self.hypergraph.part_vert_filtered[i] for t_idx in                     rep_dict[v]])            else:  # counts net connectivities only once                visited_traits = []                for v in self.hypergraph.part_vert_filtered[i]:                    for t_idx in rep_dict[v]:                        if t_idx not in visited_traits:                            visited_traits.append(t_idx)                conn_counter = Counter([self.hypergraph.net_conn[t_idx] for t_idx in visited_traits])            conns_counters.append(conn_counter)        # Plot connectivities for each part        X_axis = np.arange(self.hypergraph.nr_parts)        conn_1_data = [conns_counters[0][1], conns_counters[1][1], conns_counters[2][1]]        plt.bar(X_axis - 0.2, conn_1_data, 0.2,                label='Connectivity 1')        print("Connectivity 1 -- part 1, 2 and 3:", conns_counters[0][1], conns_counters[1][1], conns_counters[2][1])        conn_2_data = [conns_counters[0][2], conns_counters[1][2], conns_counters[2][2]]        plt.bar(X_axis, conn_2_data, 0.2,                label='Connectivity 2')        print("Connectivity 2 -- part 1, 2 and 3:", conns_counters[0][2], conns_counters[1][2], conns_counters[2][2])        conn_3_data = [conns_counters[0][3], conns_counters[1][3], conns_counters[2][3]]        plt.bar(X_axis + 0.2, conn_3_data, 0.2,                label='Connectivity 3')        print("Connectivity 3 -- part 1, 2 and 3:", conns_counters[0][3], conns_counters[1][3], conns_counters[2][3])        plt.xticks(X_axis, Y_axis, fontsize=15)        plt.yticks(fontsize=15)        plt.xlabel("Part number", fontsize=15)        plt.ylabel("Number of traits of the NFTs in the part", fontsize=15)        plt.title('Net connectivity distribution in the partition of ' + self.hypergraph.get_name_str(), fontsize=15)        if not is_unique:            plt.ylim((0, 22500) if self.hypergraph.collection == BAYC else (0, 12000))        plt.legend(fontsize=15)        plt.show()    # Computes histograms of average generalised means/p-norms per part    def draw_histograms_avg_rarity_per_part_PaToH(self):        if not self.hypergraph.nr_parts == 3:            raise Exception("Visualisation available only for three parts resulted from PaToH.")        # Set the results for the 4 metrics        p_1 = get_p_norm_for_collection(self.hypergraph.collection, p=1, apply_log=False,                                        divide_by_n=True, infinity=NO_INFINITY)        p_minus_infinity = get_p_norm_for_collection(self.hypergraph.collection, p=0, apply_log=False,                                                     divide_by_n=False, infinity=MINUS_INFINITY)        p_plus_infinity = get_p_norm_for_collection(self.hypergraph.collection, p=0, apply_log=False, divide_by_n=False,                                                    infinity=PLUS_INFINITY)        geometric_mean = get_p_norm_for_collection(self.hypergraph.collection, p=0, apply_log=True, divide_by_n=UNUSED,                                                   infinity=NO_INFINITY)        avg_p_norms = []        Y_axis = [str(i + 1) for i in range(self.hypergraph.nr_parts)]        for i in range(self.hypergraph.nr_parts):            vs = self.hypergraph.part_vert_filtered[i]            # Compute the p-norms            p_1_values = [p_1[v] for v in vs]            p_minus_inf_values = [p_minus_infinity[v] for v in vs]            p_plus_inf_values = [p_plus_infinity[v] for v in vs]            geom_mean_values = [geometric_mean[v] for v in vs]            # Compute the average of p-norms            avg_p_1_norm = np.mean(p_1_values)            avg_p_minus_inf_norm = np.mean(p_minus_inf_values)            avg_p_plus_inf_norm = np.mean(p_plus_inf_values)            avg_p_norm_log = np.mean(geom_mean_values)            avg_p_norms.append((avg_p_1_norm, avg_p_minus_inf_norm, avg_p_plus_inf_norm, avg_p_norm_log))        X_axis = np.arange(self.hypergraph.nr_parts)        # Plot the results        plt.bar(X_axis - 0.3, [avg_p_norms[0][0], avg_p_norms[1][0], avg_p_norms[2][0]], 0.2,                label='Arithmetic mean (p = 1)')        plt.bar(X_axis - 0.1, [avg_p_norms[0][3], avg_p_norms[1][3], avg_p_norms[2][3]], 0.2,                label='Negative logarithm of geometric mean (p = 0)')        plt.bar(X_axis + 0.1, [avg_p_norms[0][1], avg_p_norms[1][1], avg_p_norms[2][1]], 0.2,                label='Minimum (p -> -inf)')        plt.bar(X_axis + 0.3, [avg_p_norms[0][2], avg_p_norms[1][2], avg_p_norms[2][2]], 0.2,                label='Maximum (p -> +inf)')        plt.ylim((0, 1.9))        plt.xticks(X_axis, Y_axis, fontsize=15)        plt.yticks(fontsize=15)        plt.xlabel("Part number", fontsize=15)        plt.ylabel("Average of the norms of the trait probabilities", fontsize=15)        plt.title('Average rarity results for ' + self.hypergraph.get_name_str(), fontsize=15)        plt.legend(fontsize=15)        plt.show()    # Computes histograms of average and median prices for the last sales and average sales prices    def draw_histograms_prices_per_part_PaToH(self):        if not self.hypergraph.nr_parts == 3:            raise Exception("Visualisation available only for 3 parts")        prices = []        Y_axis = [str(i + 1) for i in range(self.hypergraph.nr_parts)]        for i in range(3):            vs = self.hypergraph.part_vert_filtered[i]            # Get the prices            last_sale_prices = [self.last_sale_prices[v] for v in vs]            avg_prices = [self.average_prices[v] for v in vs]            # Set the median and average of prices            avg_last_sale_prices = sum(last_sale_prices) / len(last_sale_prices)            avg_avg_prices = sum(avg_prices) / len(avg_prices)            last_sale_median = np.median(last_sale_prices)            avg_price_median = np.median(avg_prices)            prices.append((avg_last_sale_prices, avg_avg_prices, last_sale_median, avg_price_median))        X_axis = np.arange(3)        plt.bar(X_axis - 0.3, [prices[0][0], prices[1][0], prices[2][0]], 0.2,                label='Average of last sale prices')        print("Average of last sale prices: ", str([prices[0][0], prices[1][0], prices[2][0]]))        plt.bar(X_axis - 0.1, [prices[0][1], prices[1][1], prices[2][1]], 0.2,                label='Average of average sale prices')        print("Average of average sale prices: ", str([prices[0][1], prices[1][1], prices[2][1]]))        plt.bar(X_axis + 0.1, [prices[0][2], prices[1][2], prices[2][2]], 0.2,                label='Median of last sale prices')        print("Median of last sale prices: ", str([prices[0][2], prices[1][2], prices[2][2]]))        plt.bar(X_axis + 0.3, [prices[0][3], prices[1][3], prices[2][3]], 0.2,                label='Median of average sale prices')        print("Median of average sale prices: ", str([prices[0][3], prices[1][3], prices[2][3]]))        plt.xticks(X_axis, Y_axis, fontsize=15)        plt.yticks(fontsize=15)        plt.xlabel("Part number", fontsize=15)        plt.ylabel("Price", fontsize=15)        plt.title('Price results for ' + self.hypergraph.get_name_str(), fontsize=15)        plt.legend(fontsize=15)        plt.show()    # Draws distribution of generalised means/p-norms per part (overlapped)    def draw_rarity_distribution_per_part_PaToH(self, p_norm, divide_by_n, apply_log, infinity):        if not self.hypergraph.nr_parts == 3:            raise Exception("Visualisation available only for 3 parts")        if self.hypergraph.by_price:            raise Exception("Visualisation available only for the partitioning produced by PaToH.")        p_norms = get_p_norm_for_collection(self.hypergraph.collection, p=p_norm, apply_log=apply_log,                                            divide_by_n=divide_by_n, infinity=infinity)        colors = ['red', 'orange', 'green']        # Determine the range of values for x axis        mean_type = helper_determine_mean_for_rarity_distributon(p_norm=p_norm, divide_by_n=divide_by_n, apply_log=apply_log, infinity=infinity)        if mean_type == ARITHMETIC_MEAN:            x_range = [0, 0.175] if self.hypergraph.collection == BAYC else [0, 0.605]        elif mean_type == GEOMETRIC_MEAN:            x_range = [0.5, 2] if self.hypergraph.collection == BAYC else [0, 2.5]        else:            x_range = [0, 3]    # Not used        bins_undefined = True        bins = 0        all_p_values = []        for i in range(3):            vs = self.hypergraph.part_vert_filtered[i]            p_values = [p_norms[v] for v in vs]            all_p_values += p_values            if not bins_undefined:                plt.hist(x=p_values, bins=bins, alpha=0.4, edgecolor='black', label='Part ' + str(i + 1),                         color=colors[i])            else:                bins_undefined = False                _, bins, _ = plt.hist(x=p_values, bins=100, alpha=0.4, edgecolor='black', label='Part ' + str(i + 1),                                      color=colors[i], range=x_range)            print("(Part " + str(i + 1) + ") Mean:", np.mean(p_values), "std deviation:", np.std(p_values))            plt.axvline(np.mean(p_values), color=colors[i], linestyle='dashed', linewidth=1, alpha=0.8)        print("(All parts) Mean:", np.mean(all_p_values), "standard deviation:", np.std(all_p_values))        plt.ylabel("Frequency", fontsize=15)        if mean_type == ARITHMETIC_MEAN:            plt.xlabel("Arithmetic mean (p=1) of the trait probabilities", fontsize=15)        elif mean_type == GEOMETRIC_MEAN:            plt.xlabel("Negative logarithm of the geometric mean (p=0) of the trait probabilities", fontsize=15)        else:            plt.xlabel("Rarity", fontsize=15)        plt.title("Rarity distribution for " + str(self.hypergraph.get_name_str()), fontsize=15)        plt.yticks(fontsize=15)        plt.xticks(fontsize=15)        plt.legend(fontsize=15)        plt.show()    # Draws distribution of prices    def draw_price_distribution_PaToH(self, price_type):        all_vs = self.last_sale_prices.keys()        # Draw price distributions overlapped if both        if price_type == PRICE_HISTOGRAM_BOTH:            last_sales = [log10(self.last_sale_prices[v]) for v in all_vs]            avg_sales = [log10(self.average_prices[v]) for v in all_vs]            _, bins, _ = plt.hist(x=last_sales, bins=100, alpha=0.6, label='Last sale price', range=[0.25, 7.5])            plt.hist(x=avg_sales, bins=bins, alpha=0.6, label='Average sale price')            plt.xlabel('Log Base 10 of price', fontsize=15)            plt.legend(loc='upper left', fontsize=15)        else:            if price_type == PRICE_HISTOGRAM_LAST_SALE:                prices = [log10(v) for v in self.last_sale_prices.values()]            else:                prices = [log10(v) for v in self.average_prices.values()]            plt.hist(x=prices, bins=100, color='blue', edgecolor='black', range=[0.25, 7.5])            plt.xlabel('Log Base 10 of ' + ('last sale price' if price_type == PRICE_HISTOGRAM_LAST_SALE                                            else 'average sale price'))        plt.ylabel("Frequency", fontsize=15)        plt.xticks(fontsize=15)        plt.yticks(fontsize=15)        plt.title('Price distribution for ' + self.hypergraph.get_name_str(), fontsize=15)        plt.show()# Prints the price partitioning running times already documenteddef print_price_partitioning_running_times():    two_parts_1000 = [0.476, 0.5, 0.519, 0.532, 0.5435, 0.577, 0.595, 0.593, 0.636,                      0.607]  # 2 parts, 1000 elements, imb 1%-10%    three_parts_1000 = [0.55, 0.739, 1.01, 1.525, 1.961, 2.564, 3.493, 4.198, 5.058,                        6.242]  # 3 parts, 1000 elements, imb 1%-10%    two_parts_5560 = [0.614, 0.804, 0.956, 1.102, 1.285, 1.463, 1.606, 1.784, 1.961,                      2.072]  # 2 parts, 5560 elements, imb 1%-10%    three_parts_5560 = [3.911, 14.258, 29.838, 51.146, 80.488, 115.011, 150.891, 199.361, 249.979,                        311.915]  # 3 parts, 5560 elements, imb 1%-10%    two_parts_8621 = [1.248, 1.72, 2.206, 2.683, 3.24, 3.7, 4.194, 4.615, 5.125,                      5.64]  # 2 parts, 8621 elements, imb 1%-10%    three_parts_8621 = [15.69, 59.83, 133.053, 230.198, 356.491]  # 3 parts, 8621 elements, imb 1%-5%    x_imb = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]    labels = ["K=2, N=1000", "K=3, N=1000", "K=2, N=5560", "K=3, N=5560", "K=2, N=8621"]    y = [two_parts_1000, three_parts_1000, two_parts_5560, three_parts_5560, two_parts_8621]    for i in range(len(y)):        plt.plot(x_imb, y[i], label=labels[i])        plt.scatter(x_imb, y[i])    plt.plot([1, 2, 3, 4, 5], three_parts_8621, label="K=3, N=8621")    plt.scatter([1, 2, 3, 4, 5], three_parts_8621)    plt.ylabel("Running time (s)", fontsize=15)    plt.xlabel("Maximum imbalance allowed (%)", fontsize=15)    plt.title("Running times for the price partitioning algorithm (imbalance from 1% to 10%)")    plt.xticks(fontsize=15)    plt.yticks(fontsize=15)    plt.legend(fontsize=15)    plt.show()def helper_get_part_center(parts, by_price):    assert 1 <= parts <= 4    if parts == 4:        if by_price:            return [-0.75, 0.75, -0.75, 0.75], [0.75, 0.75, -0.75, -0.75]        return [-1, 1, -1, 1], [1, 1, -1, -1]    if parts == 3:        if by_price:            return [-0.75, 0.75, 0], [0.75, 0.75, -1.057]        return [-1, 1, 0], [1, 1, -1.4]    if parts == 2:        if by_price:            return [-0.75, 0.75], [0, 0]        return [-1, 1], [0, 0]    raise Exception("Number of parts " + str(parts) + " is not implemented for visualisation.")# Helper for draw_graph; computes the positions of the vertices in each partdef helper_draw_subgraph_around(pos, radius, nodes, graph):    graph.subgraph(nodes)    curr_angle = 0    angle_sector = 2 * math.pi / len(nodes)    positions = dict()    for n in nodes:        x = float('{0:.2f}'.format(radius * math.cos(curr_angle))) + pos[0]        y = float('{0:.2f}'.format(radius * math.sin(curr_angle))) + pos[1]        curr_angle += angle_sector        positions[n] = (x, y)    return positions# Nodes can be coloured by last sale price, number of trades and min, max prices of the partdef helper_construct_legend(node_color_type, is_price_filtered, by_price, legend, ax):    # Construct the labels of the legends    for i in legend.keys():        if node_color_type == COLOR_NODES_LAST_SALE:            label_str = "NFTs with price <= $" + str(legend[i])        elif node_color_type == COLOR_NODES_NR_TRADES:            label_str = "NFTs with no. of sales <= " + str(legend[i])        else:            label_str = "NFTs with price within " + str(legend[i])        ax.plot([0], [0], color=COLOR_DICT[i], label=label_str, scalex=False, scaley=False)    # Set the titles and sizes of the legends    if node_color_type == COLOR_NODES_PART_PRICE:        ax.plot([0], [0], color=COLOR_INT_NET, label="Internal nets", scalex=False, scaley=False)        lg = ax.legend(loc=4)        lg.set_title('Price ranges for the NFTs of each partition', prop={'size': 'large'})    elif node_color_type == COLOR_NODES_LAST_SALE:        if not is_price_filtered:            ax.plot([0], [0], color=GRAY_COLOR, label="NFTs without price considered", scalex=False, scaley=False)        ax.plot([0], [0], color=COLOR_INT_NET, label="Internal nets", scalex=False, scaley=False)        if not by_price:            ax.plot([0], [0], color=COLOR_EXT_NET, label="External nets", scalex=False, scaley=False)        lg = ax.legend(loc=4, fontsize=13)        lg.set_title('Node colours', prop={'size': 'large'})    else:  # By number of trades        ax.plot([0], [0], color=GRAY_COLOR, label="NFTs without sales considered", scalex=False, scaley=False)        ax.plot([0], [0], color=COLOR_INT_NET, label="Internal nets", scalex=False, scaley=False)        if not by_price:            ax.plot([0], [0], color=COLOR_EXT_NET, label="External nets", scalex=False, scaley=False)        lg = ax.legend(loc=4)        lg.set_title('Node colours', prop={'size': 'large'})# Calculates Kendall coefficient with the stats librarydef calculate_correlation_Kendall(x, y):    coeff, p = stats.kendalltau(x, y)    print('Kendall correlation coefficient: %.3f' % coeff)    # Interpret the significance    alpha = 0.05    if p > alpha:        print('Samples are uncorrelated -- Kendall -- (fail to reject H0) p=%.4f' % p, "(", p, ")")    else:        print('Samples are correlated -- Kendall -- (reject H0) p=%.4f' % p, "(", p, ")")# Calculates Spearman coefficient with the stats librarydef calculate_correlation_Spearman(x, y):    coeff, p = spearmanr(x, y)    print('Spearman correlation coefficient: %.3f' % coeff)    # Interpret the significance    alpha = 0.05    if p > alpha:        print('Samples are uncorrelated -- Spearman -- (fail to reject H0) p=%.4f' % p)    else:        print('Samples are correlated -- Spearman -- (reject H0) p=%.4f' % p)# Returns the type of meandef helper_determine_mean_for_rarity_distributon(p_norm, divide_by_n, apply_log, infinity):    if p_norm == 1 and divide_by_n and (not apply_log) and infinity == NO_INFINITY:        return ARITHMETIC_MEAN    if p_norm == 0 and (not divide_by_n) and apply_log and infinity == NO_INFINITY:        return GEOMETRIC_MEAN    return OTHER_MEAN